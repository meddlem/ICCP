\Chapter{Optimization}{(or, where you \emph{shouldn't} spend 90\% of your time)}
%\chapter[Optimization]{Optimization (or where you \emph{shouldn't} spend 90\% of your time)}
\label{chap:Optimization}

\begin{quote}\small
\emph{``Premature optimization is the root of all evil.''} \\ \hspace*{\fill}---Donald E.\ Knuth
\end{quote}
\begin{quote}\small
\emph{``Rules of Optimization: \\ Rule 1: Don't do it. \\ Rule 2 (for experts only): Don't do it yet.''} \\ \hspace*{\fill}---Michael A.\ Jackson
\end{quote}
Impatience is the most important characteristic of a good programmer.
However, this does result in a tendency to focus too much or too early on optimizing code. Don't.
Always make sure your code is correct and readable before you try to make it faster.
And when you do try to make it faster, conventional wisdom says: \emph{don't optimize your code, optimize your algorithms.}
This may seen counter-intuitive; after all, what's wrong with fast code?
However, modern CPUs are complex beasts, and unless you're an expert, it's not at all obvious which expressions will result in the fastest code.
This also means that a lot of tricks that used to be valid ten years ago no longer are.
Let's look at a few examples:
\begin{itemize}
  \item\textbf{Division is slower than multiplication by an inverse.} Although true, optimizing for this results in code that is hard to read, and possibly less accurate.
    Besides, when you're using the \texttt{-ffast-math} option, \texttt{gfortran} will do this for you.
    A similar observation holds for exponentiation; it is slower than multiplication in general, but \texttt{gfortran} will automatically convert \texttt{x**2} to \texttt{x * x}, provided the exponent is \keyword{integer}. \newpage
  \item\textbf{Unwinding small loops reduces overhead.} Common wisdom says that
\begin{lstlisting}[caption=, nolol]
  do i = 1, 3
      call do_something(i)
  end do
\end{lstlisting}
    is slower than
\begin{lstlisting}[caption=, nolol]
  call do_something(1)
  call do_something(2)
  call do_something(3)
\end{lstlisting}
    because of the loop's overhead.
    Although again true in principle, modern CPUs use branch prediction to dramatically reduce this overhead.
    Moreover, unwinding loops results in more machine code, which might actually hurt performance if it no longer fits in the CPU instruction cache.
    When you compile with \texttt{-march=native}, the compiler knows about the CPU limitations and will decide whether loop unwinding is the smart thing to do.
  \item\textbf{Calculate everything only once.} Let's say that $x^2$ occurs four times in your expression, then it makes sense to calculate \texttt{x2 = x**2} separately and use the result four times, right? Possibly yes, but only if it improves readability.
    Common subexpression elimination is something compilers have done for ages, so again \texttt{gfortran} will do this for you.
\end{itemize}
There are countless more examples, but I think you get the idea.
In short: optimize for readability; write clean code and let the compiler worry about the rest.
Unless you're intimately familiar with things like cache misses, branch prediction and vector instructions, you won't be able to outsmart the compiler.

\section{Some optimizations you are allowed to use}

That being said, there are a few general rules that will prevent your code from accidentally becoming horrendously slow:
\begin{itemize}
  \item\textbf{Don't reinvent the wheel.} Don't try to write your own matrix multiplication routine, use \keyword{matmul}, or the routines from BLAS (see \nameref{chap:Linear algebra}).
    Fortran has many built-in numerical functions that are much faster than anything you'll be able to write---use them! (Google `Fortran intrinsics' to get an overview.)
    \newpage
  \item\textbf{Use array operations.} You can add two arrays by writing
\begin{lstlisting}[caption=, nolol]
  do i = 1, n
      do j = 1, m
          c(i, j) = a(i, j) + b(i, j)
      end do
  end do
\end{lstlisting}
or simply by writing \texttt{c = a + b}.
Guess which is both faster\marginnote{It's easy to forget that array operations make use of a loop ``under the hood,'' though the compiler will generally implement this loop as efficiently as it can.} and more readable?
  \item\textbf{Keep inner loops small.} The code inside loops, especially in nested loops, is executed many times.
    This code will likely be the bottleneck of your program, so it makes sense to concentrate your optimization efforts there.
    Move as much code out of the inner loop as possible.
  \item\textbf{Don't use system calls in nested loops.} Certain operations, such as allocating memory, or writing to the screen or disk, require a system call---a function call to the underlying operating system.
    Such calls are expensive, so try to avoid them in nested loops.
    Especially disk I/O is slow.
    Luckily, modern operating systems are smart enough to buffer these operations, but they can still become a bottleneck.
  \item\textbf{Be smart about memory usage.} First, don't use more memory than you need, especially more memory than is physically available.
    That will result in paging (storing and retrieving data from disk), which is orders of magnitude slower than direct memory access.
    Second, think about memory layout.
    Fortran uses column-major ordering for arrays.\footnote{MATLAB and R adopt the same convention as Fortran; C, Python, and Mathematica use row-major ordering.}
    This means that the array
    \begin{equation*}
    \begin{bmatrix}
        a & b & c & d \\
        e & f & g & h \\
        i & j & k & l
    \end{bmatrix}
    \end{equation*}
    is actually stored in memory as $\left[a,e,i,b,f,j,c,g,k,d,h,l\right]$.
    Because of this, operations within a single column are much faster than operations within a single row; the processor can easily load a whole column into the cache, but not a whole row.
    It is therefore important to carefully consider your array layout.
    For example, if you have an array containing the coordinates of many particles, then a $3\times n$ layout will be more efficient than $n\times 3$.
    In the former, the $x$, $y$ and $z$ coordinates are close together in memory, meaning that operations on single particles will be fast.
\end{itemize}
These optimizations are more about preventing your code from becoming unnecessarily slow, than trying to squeeze the last drop of performance out of it.
Besides, low-level optimizations generally yield only about a factor of 2 speedup, and that's if you know what you're doing.
To see real speedups, you're better off profiling your code to determine bottlenecks and optimizing your algorithms by improving scalability.

\section{Scalability; or, the importance of big O}

If you want your program to be fast, choose your algorithms carefully.
The fact that you can approximate an integral by a simple sum does not mean that's the best way to do it.
In fact, the \naive\ approach is often the worst method, both in terms of computational complexity, \ie, how much more expensive the calculation becomes when you increase the size of the problem, and numerical accuracy, \ie, how small the error becomes when you increase the number of steps/iterations.
Both the complexity and the accuracy of an algorithm are generally quantified using \emph{big O} notation.
Big O notation is essentially a Taylor expansion of your algorithm; usually in the step size $h$ to approximate the error, or the dimension $n$ to approximate the computation time.
In general, only the highest or lowest power matters, since that will be the dominant term for large $n$ or small $h$.
Different algorithms for solving the same problem are compared by looking at their big O characteristics.
However, scaling is not everything.
Some algorithms, such as the famous Coppersmith-Winograd algorithm for matrix multiplication, scale very well, but have such a large prefactor in their computational cost, that they're inefficient for all but the largest problems.

Long story short; spend some time doing research before you start coding.
Look at different algorithms, compare their accuracies and complexities---don't forget the complexity of implementation!---and only then start writing.
Don't just go for the seemingly obvious approach.
For example, \naively\ integrating an ordinary differential equation (ODE), \eg, using the Euler method, will make an error of order $\Oh{h^2}$ at each integration step, giving a total error of $\Oh{h}$.
Moreover, this method is unstable, meaning that for stiff equations the solution will oscillate wildly and the error grow very large.
The Runge-Kutta method, on the other hand, makes an error of $\Oh{h^5}$ per step, giving a total error of $\Oh{h^4}$ (which is why it's generally referred to as `RK4').
Additionally, the method is numerically stable.
%Alternatively, the velocity Verlet algorithm has a global error of $\Oh{h^2\right)$, but it is symplectic, \ie, it is time-reversible and conserves (a discrete version of the) energy.
%This makes it ideal for, \eg, molecular dynamics simulations.
Many such methods are significantly more accurate than the Euler method, without being harder to implement.
Furthermore, better scaling of the error means that they need fewer iterations, making them also much faster; it's a win-win.

Numerical analysis is a large field, and it can be quite daunting trying to understand the merits of different methods, or even finding them.
For this, we recommend the book \emph{Numerical Recipes}\index{Numerical Recipes}\footnote{Available free online at \url{http://apps.nrbook.com/fortran/index.html}}.
It covers a wide range of topics, focuses on best practices, and does not ignore coding effort when comparing algorithms.
It even includes the full source code of the implementations! We recommend
always using it as your starting point, with one exception: \nameref{chap:Linear algebra}.

We conclude this section with an overview of the computational complexity of certain common operations.
This will give you an idea of the most likely bottleneck in your code.
\begin{itemize}
  \item All scalar operations are $\Oh{1}$, \ie, constant time.
    Integer operations are much faster than floating-point operations.
    Addition and subtraction are the fastest, followed by multiplication and finally division.
    Functions like \keyword{sqrt}, \keyword{exp}, \keyword{log}, \keyword{sin}, \keyword{cos}, \emph{etc.}, are not always available as single processor instructions, especially for complex numbers, in which case they are implemented as actual function calls.
    It goes without saying that this is much slower than basic arithmetic.
  \item Scaling a vector or a matrix means performing an operation on each element.
    The complexity is therefore $\Oh{n}$ for vectors and $\Oh{n^2}$ for matrices, where $n$ is the dimension.
  \item Vector addition and taking an inner product are $\Oh{n}$, matrix addition is $\Oh{n^2}$.
  \item Matrix-vector multiplication means taking an inner product for each row, so the complexity is $\Oh{n^2}$.
  \item Solving a system of linear equations is also $\Oh{n^2}$, but the prefactor is much larger than for matrix-vector multiplication.
  \item \Naive\ matrix-matrix multiplication is $\Oh{n^3}$.
    Libraries such as BLAS and \indexentry{LAPACK} are smarter and do it in $\Oh{n^{\log_2(7)}}\approx\Oh{n^{2.807}}$.
    Another reason to avoid writing this yourself.\footnote{The Fortran intrinsic \keyword{matmul} is actually implemented using BLAS.}
  \item Taking the inverse of\ a matrix and calculating the eigenvalues/eigenvectors are both $\Oh{n^3}$.
    However, eigendecomposition has a much larger prefactor and is therefore significantly slower.
    It is logically the most expensive matrix operation, since all operations on a diagonal matrix are $\Oh{n}$.
  \item Iterative algorithms like conjugate-gradient or Krylov-subspace methods are supra-convergent in the number of iterations, \ie, the error decreases by a constant factor at each iteration.
    They are especially suitable for sparse matrices, since each iteration is $\Oh{n}$, where $n$ is the number of non-zero elements instead of the dimension.
\end{itemize}


